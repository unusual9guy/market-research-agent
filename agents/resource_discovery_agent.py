"""
Resource Discovery Agent for Multi-Agent Market Research System

This agent discovers and curates relevant resources (datasets, code repositories, 
papers, tools) for AI use cases generated by the Use Case Generation Agent.
"""

import logging
import requests
import os
from typing import Dict, List, Any, Optional
from langchain_openai import ChatOpenAI
from utils.exa_searcher import ExaSearcher
from config import config
from models.resource_models import DatasetResource, UseCaseResources, ResourceDiscoveryResult

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ResourceDiscoveryAgent:
    """
    Agent responsible for discovering datasets for AI use cases.
    
    For each use case, finds:
    - Relevant datasets (Kaggle, HuggingFace, Google Dataset Search)
    - Focuses only on datasets for clean, focused output
    """
    
    def __init__(self):
        """Initialize the Resource Discovery Agent."""
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0.3,
            api_key=config.OPENAI_API_KEY
        )
        self.web_searcher = ExaSearcher()
        self.github_token = config.GITHUB_TOKEN
        
    def discover_resources_for_use_cases(self, use_cases: List[Dict[str, Any]]) -> ResourceDiscoveryResult:
        """
        Discover datasets for a list of AI use cases.
        
        Args:
            use_cases: List of use case dictionaries from Agent 2
            
        Returns:
            ResourceDiscoveryResult with structured dataset discoveries
        """
        try:
            logger.info(f"Starting dataset discovery for {len(use_cases)} use cases")
            
            use_case_resources_list = []
            total_datasets = 0
            
            for i, use_case in enumerate(use_cases, 1):
                use_case_title = use_case.get('title', f'Use Case {i}')
                use_case_description = use_case.get('ai_solution', 'No description available')
                logger.info(f"Discovering datasets for: {use_case_title}")
                
                # Discover datasets for this specific use case
                datasets = self._discover_datasets_for_use_case(use_case)
                
                # Create structured resource object
                use_case_resource = UseCaseResources(
                    use_case_title=use_case_title,
                    use_case_description=use_case_description,
                    datasets=datasets,
                    total_datasets=len(datasets)
                )
                
                use_case_resources_list.append(use_case_resource)
                total_datasets += len(datasets)
                
                logger.info(f"Found {len(datasets)} datasets for {use_case_title}")
            
            # Create result object
            result = ResourceDiscoveryResult(
                status='completed',
                total_use_cases=len(use_cases),
                total_datasets_found=total_datasets,
                use_case_resources=use_case_resources_list,
                search_sources_used=['Kaggle', 'HuggingFace', 'Exa Web Search']
            )
            
            # Save to markdown file
            self._save_to_markdown(result)
            
            return result
            
        except Exception as e:
            logger.error(f"Error during resource discovery: {e}")
            return ResourceDiscoveryResult(
                status='error',
                total_use_cases=0,
                total_datasets_found=0,
                use_case_resources=[],
                search_sources_used=[]
            )
    
    def _discover_datasets_for_use_case(self, use_case: Dict[str, Any]) -> List[DatasetResource]:
        """
        Discover datasets for a single use case.
        
        Args:
            use_case: Single use case dictionary
            
        Returns:
            List of DatasetResource objects
        """
        use_case_title = use_case.get('title', 'Unknown Use Case')
        ai_solution = use_case.get('ai_solution', '')
        strategic_value = use_case.get('strategic_value', '')
        
        # Combine context for better search queries
        search_context = f"{use_case_title} {ai_solution} {strategic_value}"
        
        try:
            # Discover datasets using multiple search strategies
            datasets = []
            
            # 1. General dataset search
            datasets.extend(self._search_general_datasets(search_context, use_case_title))
            
            # 2. Kaggle-specific search
            datasets.extend(self._search_kaggle_datasets(search_context))
            
            # 3. HuggingFace-specific search
            datasets.extend(self._search_huggingface_datasets(search_context))
            
            # 4. Domain-specific searches
            datasets.extend(self._search_domain_specific_datasets(search_context, use_case_title))
            
            # Filter and rank datasets
            datasets = self._filter_and_rank_datasets(datasets)
            
            return datasets[:10]  # Limit to top 10 datasets per use case
            
        except Exception as e:
            logger.error(f"Error discovering datasets for {use_case_title}: {e}")
            return []
    
    def _search_general_datasets(self, search_context: str, use_case_title: str) -> List[DatasetResource]:
        """Search for general datasets related to the use case."""
        logger.info(f"Searching for general datasets related to: {use_case_title}")
        
        datasets = []
        
        try:
            # Search for datasets using targeted queries
            dataset_queries = [
                f"{search_context} dataset training data",
                f"{use_case_title} machine learning dataset",
                f"{search_context} open dataset",
                f"{use_case_title} AI dataset"
            ]
            
            for query in dataset_queries:
                search_results = self.web_searcher.search_ai_use_cases(query)
                
                # Exa returns a list directly, not a dict with 'results' key
                for result in search_results[:2]:  # Limit to top 2 per query
                    if self._is_dataset_resource(result):
                        dataset_resource = self._create_dataset_resource(result)
                        if dataset_resource and dataset_resource not in datasets:
                            datasets.append(dataset_resource)
            
        except Exception as e:
            logger.error(f"Error in general dataset search: {e}")
        
        return datasets
    
    def _search_domain_specific_datasets(self, search_context: str, use_case_title: str) -> List[DatasetResource]:
        """Search for domain-specific datasets based on use case type."""
        logger.info(f"Searching for domain-specific datasets for: {use_case_title}")
        
        datasets = []
        
        try:
            # Extract domain-specific keywords from use case title
            title_lower = use_case_title.lower()
            
            if 'recommendation' in title_lower:
                domain_queries = [
                    f"recommendation system dataset user ratings",
                    f"collaborative filtering dataset",
                    f"movie recommendation dataset"
                ]
            elif 'image' in title_lower or 'vision' in title_lower:
                domain_queries = [
                    f"image classification dataset",
                    f"computer vision dataset",
                    f"image recognition dataset"
                ]
            elif 'nlp' in title_lower or 'text' in title_lower or 'language' in title_lower:
                domain_queries = [
                    f"natural language processing dataset",
                    f"text classification dataset",
                    f"sentiment analysis dataset"
                ]
            elif 'time' in title_lower or 'forecast' in title_lower or 'prediction' in title_lower:
                domain_queries = [
                    f"time series dataset",
                    f"forecasting dataset",
                    f"predictive analytics dataset"
                ]
            else:
                # Generic domain search
                domain_queries = [
                    f"{use_case_title} benchmark dataset",
                    f"{search_context} standard dataset"
                ]
            
            for query in domain_queries:
                search_results = self.web_searcher.search_ai_use_cases(query)
                
                for result in search_results[:2]:
                    if self._is_dataset_resource(result):
                        dataset_resource = self._create_dataset_resource(result)
                        if dataset_resource and dataset_resource not in datasets:
                            datasets.append(dataset_resource)
            
        except Exception as e:
            logger.error(f"Error in domain-specific dataset search: {e}")
        
        return datasets
    
    def _create_dataset_resource(self, result: Dict[str, Any]) -> DatasetResource:
        """Create a DatasetResource object from search result."""
        try:
            title = result.get('title', 'Untitled Dataset')
            description = result.get('content', '')[:300] + '...' if result.get('content') else 'No description available'
            url = result.get('url', '')
            source = self._identify_source(url)
            
            # Extract additional metadata
            size = self._extract_size_from_description(description)
            format_type = self._extract_format_from_description(description)
            tags = self._extract_tags_from_description(description, title)
            
            return DatasetResource(
                title=title,
                description=description,
                url=url,
                source=source,
                relevance_score=0.8,  # Default high relevance for found datasets
                size=size,
                format=format_type,
                tags=tags
            )
        except Exception as e:
            logger.error(f"Error creating dataset resource: {e}")
            return None
    
    def _filter_and_rank_datasets(self, datasets: List[DatasetResource]) -> List[DatasetResource]:
        """Filter and rank datasets by relevance and quality."""
        if not datasets:
            return []
        
        # Remove duplicates based on URL
        seen_urls = set()
        unique_datasets = []
        for dataset in datasets:
            if dataset.url and dataset.url not in seen_urls:
                seen_urls.add(dataset.url)
                unique_datasets.append(dataset)
        
        # Sort by relevance score (descending)
        unique_datasets.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return unique_datasets
    
    def _save_to_markdown(self, result: ResourceDiscoveryResult) -> str:
        """Save the resource discovery result to a markdown file."""
        try:
            # Create agent3_output directory if it doesn't exist
            output_dir = "agent3_output"
            os.makedirs(output_dir, exist_ok=True)
            
            # Generate filename with timestamp
            timestamp = result.discovery_timestamp.strftime("%Y%m%d_%H%M%S")
            filename = f"{output_dir}/resource_discovery_{timestamp}.md"
            
            # Generate markdown content
            markdown_content = self._generate_markdown_report(result)
            
            # Save to file
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            logger.info(f"Resource discovery report saved to: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"Error saving markdown report: {e}")
            return None
    
    def _generate_markdown_report(self, result: ResourceDiscoveryResult) -> str:
        """Generate markdown content for the resource discovery report."""
        markdown = f"""# Dataset Discovery Report

**Generated:** {result.discovery_timestamp.strftime('%Y-%m-%d %H:%M:%S')}  
**Total Use Cases:** {result.total_use_cases}  
**Total Datasets Found:** {result.total_datasets_found}  
**Sources Used:** {', '.join(result.search_sources_used)}

---

"""
        
        for i, use_case_resource in enumerate(result.use_case_resources, 1):
            markdown += f"""## {i}. {use_case_resource.use_case_title}

**Description:** {use_case_resource.use_case_description}

**Datasets Found:** {use_case_resource.total_datasets}

"""
            
            if use_case_resource.datasets:
                for j, dataset in enumerate(use_case_resource.datasets, 1):
                    markdown += f"""### {j}. {dataset.title}

- **Source:** {dataset.source}
- **URL:** [{dataset.url}]({dataset.url})
- **Relevance Score:** {dataset.relevance_score:.2f}
- **Description:** {dataset.description}
"""
                    
                    if dataset.size:
                        markdown += f"- **Size:** {dataset.size}\n"
                    
                    if dataset.format:
                        markdown += f"- **Format:** {dataset.format}\n"
                    
                    if dataset.tags:
                        markdown += f"- **Tags:** {', '.join(dataset.tags)}\n"
                    
                    markdown += "\n"
            else:
                markdown += "*No datasets found for this use case.*\n\n"
            
            markdown += "---\n\n"
        
        return markdown
    
    def _extract_size_from_description(self, description: str) -> Optional[str]:
        """Extract dataset size from description."""
        import re
        size_patterns = [
            r'(\d+(?:\.\d+)?)\s*(GB|MB|KB|TB)',
            r'(\d+(?:,\d+)*)\s*(?:rows?|records?|samples?)',
            r'(\d+(?:\.\d+)?)\s*million',
            r'(\d+(?:\.\d+)?)\s*billion'
        ]
        
        for pattern in size_patterns:
            match = re.search(pattern, description, re.IGNORECASE)
            if match:
                return match.group(0)
        
        return None
    
    def _extract_format_from_description(self, description: str) -> Optional[str]:
        """Extract dataset format from description."""
        formats = ['CSV', 'JSON', 'Parquet', 'Excel', 'SQL', 'XML', 'HDF5', 'Pickle', 'TFRecord']
        description_upper = description.upper()
        
        for format_type in formats:
            if format_type in description_upper:
                return format_type
        
        return None
    
    def _extract_tags_from_description(self, description: str, title: str) -> List[str]:
        """Extract relevant tags from description and title."""
        combined_text = f"{title} {description}".lower()
        
        # Common ML/AI tags
        tag_keywords = {
            'machine learning': ['ml', 'machine learning', 'ai'],
            'deep learning': ['deep learning', 'neural network', 'cnn', 'rnn', 'transformer'],
            'computer vision': ['image', 'vision', 'computer vision', 'cv'],
            'nlp': ['nlp', 'natural language', 'text', 'language'],
            'recommendation': ['recommendation', 'collaborative filtering'],
            'time series': ['time series', 'temporal', 'forecasting'],
            'classification': ['classification', 'classify'],
            'regression': ['regression', 'predict'],
            'clustering': ['clustering', 'cluster', 'unsupervised']
        }
        
        found_tags = []
        for tag, keywords in tag_keywords.items():
            if any(keyword in combined_text for keyword in keywords):
                found_tags.append(tag)
        
        return found_tags[:5]  # Limit to 5 tags
    
    
    def _search_kaggle_datasets(self, search_context: str) -> List[DatasetResource]:
        """Search for Kaggle datasets."""
        datasets = []
        
        try:
            # Search for Kaggle datasets
            kaggle_query = f"{search_context} site:kaggle.com dataset"
            search_results = self.web_searcher.search_ai_use_cases(kaggle_query)
            
            for result in search_results[:3]:
                if 'kaggle.com' in result.get('url', '').lower():
                    dataset_resource = self._create_dataset_resource(result)
                    if dataset_resource:
                        dataset_resource.source = 'Kaggle'
                        dataset_resource.relevance_score = 0.9  # Higher relevance for Kaggle
                        datasets.append(dataset_resource)
        
        except Exception as e:
            logger.error(f"Error searching Kaggle datasets: {e}")
        
        return datasets
    
    def _search_huggingface_datasets(self, search_context: str) -> List[DatasetResource]:
        """Search for HuggingFace datasets."""
        datasets = []
        
        try:
            # Search for HuggingFace datasets
            hf_query = f"{search_context} site:huggingface.co dataset"
            search_results = self.web_searcher.search_ai_use_cases(hf_query)
            
            for result in search_results[:3]:
                if 'huggingface.co' in result.get('url', '').lower():
                    dataset_resource = self._create_dataset_resource(result)
                    if dataset_resource:
                        dataset_resource.source = 'HuggingFace'
                        dataset_resource.relevance_score = 0.9  # Higher relevance for HuggingFace
                        datasets.append(dataset_resource)
        
        except Exception as e:
            logger.error(f"Error searching HuggingFace datasets: {e}")
        
        return datasets
    
    def _is_dataset_resource(self, result: Dict[str, Any]) -> bool:
        """Check if a search result is a dataset resource."""
        title = result.get('title', '').lower()
        url = result.get('url', '').lower()
        
        dataset_indicators = ['dataset', 'data', 'training data', 'benchmark']
        return any(indicator in title for indicator in dataset_indicators) or \
               any(domain in url for domain in ['kaggle.com', 'huggingface.co', 'datasetsearch'])
    
    def _identify_source(self, url: str) -> str:
        """Identify the source platform from URL."""
        url_lower = url.lower()
        
        if 'kaggle.com' in url_lower:
            return 'Kaggle'
        elif 'huggingface.co' in url_lower:
            return 'HuggingFace'
        elif 'github.com' in url_lower:
            return 'GitHub'
        elif 'arxiv.org' in url_lower:
            return 'arXiv'
        elif 'paperswithcode.com' in url_lower:
            return 'Papers with Code'
        else:
            return 'Web'
